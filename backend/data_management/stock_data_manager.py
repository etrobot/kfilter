from __future__ import annotations

import logging
from datetime import date, datetime, timedelta
from typing import List, Dict, Optional

import pandas as pd
from sqlmodel import Session, select
from models import (
    engine, StockBasicInfo, DailyMarketData
)
from market_data.ths_api import uplimit10jqka, build_limit_up_map

logger = logging.getLogger(__name__)

# ç¼“å­˜æœ€æ–°äº¤æ˜“æ—¥å’Œæ¶¨åœæ•°æ®ï¼Œé¿å…é‡å¤APIè°ƒç”¨
_latest_trade_date_cache = None
_limit_map_cache = None


def get_latest_trade_date_and_limit_map(use_cache: bool = True):
    """ç»Ÿä¸€è·å–æœ€æ–°äº¤æ˜“æ—¥å’Œæ¶¨åœæ•°æ®ï¼Œå¹¶æ›´æ–°åˆ°æ•°æ®åº“ã€‚
    Args:
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤Trueã€‚è®¾ä¸ºFalseæ—¶å¼ºåˆ¶é‡æ–°è·å–ã€‚
    Returns:
        (äº¤æ˜“æ—¥æœŸ, æ¶¨åœæ˜ å°„)
    """
    global _latest_trade_date_cache, _limit_map_cache
    
    # å¦‚æœå·²æœ‰ç¼“å­˜ä¸”å…è®¸ä½¿ç”¨ç¼“å­˜ï¼Œç›´æ¥è¿”å›
    if use_cache and _latest_trade_date_cache is not None and _limit_map_cache is not None:
        return _latest_trade_date_cache, _limit_map_cache
    
    try:
        ths_df = uplimit10jqka("")
        if not ths_df.empty and "first_limit_up_time" in ths_df.columns:
            # è·å–æœ€æ–°äº¤æ˜“æ—¥
            timestamps = ths_df["first_limit_up_time"].dropna()
            if not timestamps.empty:
                latest_timestamp = timestamps.iloc[0]
                trade_date = pd.to_datetime(int(latest_timestamp), unit='s').date()
                
                # æ„å»ºæ¶¨åœæ˜ å°„
                limit_map = build_limit_up_map(ths_df)
                
                # æ›´æ–°åˆ°æ•°æ®åº“ï¼ˆå½“æ—¥æ¶¨åœæ•°æ®ï¼‰
                if limit_map:
                    _update_limit_data_to_db(trade_date, limit_map)
                
                # ç¼“å­˜ç»“æœ
                _latest_trade_date_cache = trade_date
                _limit_map_cache = limit_map
                
                logger.info(f"Got latest trade date {trade_date} with {len(limit_map)} limit-up stocks")
                return trade_date, limit_map
    except Exception as e:
        logger.warning(f"Failed to get latest trade date from THS: {e}")
    
    # APIå¤±è´¥ï¼Œä»æ•°æ®åº“è·å–æœ€æ–°äº¤æ˜“æ—¥
    with Session(engine) as session:
        latest_db_date = session.exec(
            select(DailyMarketData.date)
            .order_by(DailyMarketData.date.desc())
            .limit(1)
        ).first()
        if latest_db_date:
            _latest_trade_date_cache = latest_db_date
            _limit_map_cache = {}
            logger.info(f"Using latest trade date from database: {latest_db_date}")
            return latest_db_date, {}
    
    raise Exception("æ— æ³•è·å–æœ€æ–°äº¤æ˜“æ—¥æœŸï¼šTHS API å¤±è´¥ä¸”æ•°æ®åº“æ— æ•°æ®ã€‚è¯·æ£€æŸ¥ THS API è¿æ¥æˆ–å…ˆå¯¼å…¥åŸºç¡€æ•°æ®ã€‚")


def _update_limit_data_to_db(trade_date: date, limit_map: dict):
    """å°†å½“æ—¥æ¶¨åœæ•°æ®æ›´æ–°åˆ°æ•°æ®åº“"""
    if not limit_map:
        return
        
    updated = 0
    with Session(engine) as session:
        # è·å–å½“æ—¥æ‰€æœ‰è®°å½•
        day_records = session.exec(
            select(DailyMarketData).where(DailyMarketData.date == trade_date)
        ).all()
        
        for record in day_records:
            if record.code in limit_map:
                # æ¶¨åœè‚¡ç¥¨
                if record.limit_status != 1 or record.limit_up_text != limit_map[record.code]:
                    record.limit_status = 1
                    record.limit_up_text = limit_map[record.code]
                    updated += 1
            else:
                # éæ¶¨åœè‚¡ç¥¨
                if record.limit_status != 0:
                    record.limit_status = 0
                    record.limit_up_text = None
                    
        if updated > 0:
            session.commit()
            logger.info(f"Updated {updated} limit-up records for {trade_date}")


def clear_trade_date_cache():
    """æ¸…é™¤äº¤æ˜“æ—¥ç¼“å­˜ï¼Œå¼ºåˆ¶ä¸‹æ¬¡é‡æ–°è·å–"""
    global _latest_trade_date_cache, _limit_map_cache
    _latest_trade_date_cache = None
    _limit_map_cache = None


def save_spot_as_daily_data(spot_data: pd.DataFrame) -> int:
    """å°†å®æ—¶è¡Œæƒ…ä¿å­˜ä¸ºæ—¥Kæ•°æ®ï¼Œå¹¶è¡¥å……åŒèŠ±é¡ºæ¶¨åœç±»å‹æ–‡æœ¬ã€‚
    - spot_data: åŒ…å«æ—¥æœŸå­—æ®µå’Œ ['ä»£ç ','åç§°','æœ€æ–°ä»·','æ¶¨è·Œå¹…','æœ€é«˜','æœ€ä½','ä»Šå¼€','æˆäº¤é‡','æˆäº¤é¢'] çš„ DataFrame
    """
    if spot_data is None or spot_data.empty:
        return 0

    # è·å–æ¶¨åœç±»å‹æ˜ å°„
    limit_map: Dict[str, str] = {}
    try:
        ths_df = uplimit10jqka("")
        limit_map = build_limit_up_map(ths_df)
    except Exception as e:
        logger.warning(f"Failed to fetch limit-up map for spot: {e}")

    total_saved = 0
    with Session(engine) as session:
        for _, row in spot_data.iterrows():
            code = row["ä»£ç "] if "ä»£ç " in row else None
            if not code:
                continue

            # ä½¿ç”¨æ•°æ®ä¸­çš„æ—¥æœŸ
            if "æ—¥æœŸ" not in row or pd.isna(row["æ—¥æœŸ"]):
                logger.warning(f"No date found for {code}, skipping")
                continue
            trade_date = pd.to_datetime(row["æ—¥æœŸ"]).date()

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å½“æ—¥è®°å½•
            existing = session.exec(
                select(DailyMarketData).where(
                    DailyMarketData.code == code,
                    DailyMarketData.date == trade_date
                )
            ).first()

            if existing is not None:
                # å·²å­˜åœ¨åˆ™è·³è¿‡ï¼ˆå¯è€ƒè™‘æ›´æ–°ï¼‰
                continue

            change_pct = round(float(row["æ¶¨è·Œå¹…"]), 2) if "æ¶¨è·Œå¹…" in row and pd.notna(row["æ¶¨è·Œå¹…"]) else 0
            # ç›´æ¥ä½¿ç”¨åŒèŠ±é¡ºæ¶¨åœæ± æ•°æ®åˆ¤æ–­æ¶¨åœçŠ¶æ€
            limit_status = 1 if code in limit_map else 0
            limit_text = limit_map.get(code) if limit_status == 1 else None

            daily = DailyMarketData(
                code=code,
                date=trade_date,
                open_price=float(row["ä»Šå¼€"] if "ä»Šå¼€" in row else 0),
                high_price=float(row["æœ€é«˜"] if "æœ€é«˜" in row else 0),
                low_price=float(row["æœ€ä½"] if "æœ€ä½" in row else 0),
                close_price=float(row["æœ€æ–°ä»·"] if "æœ€æ–°ä»·" in row else 0),
                volume=float(row["æˆäº¤é‡"] if "æˆäº¤é‡" in row else 0),
                amount=float(row["æˆäº¤é¢"] if "æˆäº¤é¢" in row else 0),
                change_pct=change_pct,
                limit_status=limit_status,
                limit_up_text=limit_text,
            )
            session.add(daily)
            total_saved += 1

        session.commit()

    logger.info(f"Saved {total_saved} spot rows as daily data for {trade_date}")
    return total_saved


def get_missing_daily_data(stock_codes: List[str]) -> Dict[str, date]:
    """æ£€æŸ¥å“ªäº›è‚¡ç¥¨çš„æ—¥Kæ•°æ®ç¼ºå¤±æˆ–ä¸å®Œæ•´ï¼Œè¿”å›éœ€è¦ä»å“ªä¸ªæ—¥æœŸå¼€å§‹è¡¥å……æ•°æ®"""
    missing_data = {}
    
    # è·å–æœ€æ–°äº¤æ˜“æ—¥
    latest_trade_date, _ = get_latest_trade_date_and_limit_map()
    
    with Session(engine) as session:
        for code in stock_codes:
            # æŸ¥è¯¢è¯¥è‚¡ç¥¨æœ€æ–°çš„æ—¥æœŸ
            stmt = select(DailyMarketData.date).where(
                DailyMarketData.code == code
            ).order_by(DailyMarketData.date.desc()).limit(1)
            
            result = session.exec(stmt).first()
            
            if result is None:
                # è¯¥è‚¡ç¥¨æ²¡æœ‰ä»»ä½•æ•°æ®ï¼Œä»60å¤©å‰å¼€å§‹è·å–
                missing_data[code] = latest_trade_date - timedelta(days=60)
            else:
                # æ£€æŸ¥æœ€æ–°æ—¥æœŸæ˜¯å¦æ˜¯æœ€æ–°äº¤æ˜“æ—¥
                latest_date = result
                if latest_date < latest_trade_date:
                    # æ•°æ®ä¸æ˜¯æœ€æ–°çš„ï¼Œä»æœ€æ–°æ—¥æœŸçš„ä¸‹ä¸€å¤©å¼€å§‹è¡¥å……
                    missing_data[code] = latest_date + timedelta(days=1)
    
    return missing_data


def save_daily_data(history_data: Dict[str, pd.DataFrame]):
    """ä¿å­˜æ—¥Kæ•°æ®åˆ°æ•°æ®åº“"""
    total_saved = 0

    with Session(engine) as session:
        for code, df in history_data.items():
            if df is None or df.empty:
                continue

            for _, row in df.iterrows():
                record_date = row["æ—¥æœŸ"].date()
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = session.exec(
                    select(DailyMarketData).where(
                        DailyMarketData.code == code,
                        DailyMarketData.date == record_date
                    )
                ).first()

                if existing is None:
                    # Handle NaN values properly - pandas NaN should be replaced with 0
                    def safe_float(value, default=0.0):
                        """Convert value to float, replacing NaN with default"""
                        return default if pd.isna(value) else float(value)
                    
                    daily_data = DailyMarketData(
                        code=code,
                        date=record_date,
                        open_price=safe_float(row["å¼€ç›˜"] if "å¼€ç›˜" in row else 0),
                        high_price=safe_float(row["æœ€é«˜"] if "æœ€é«˜" in row else 0),
                        low_price=safe_float(row["æœ€ä½"] if "æœ€ä½" in row else 0),
                        close_price=safe_float(row["æ”¶ç›˜"] if "æ”¶ç›˜" in row else 0),
                        volume=safe_float(row["æˆäº¤é‡"] if "æˆäº¤é‡" in row else 0),
                        amount=safe_float(row["æˆäº¤é¢"] if "æˆäº¤é¢" in row and pd.notna(row["æˆäº¤é¢"]) else None),
                        change_pct=round(safe_float(row["æ¶¨è·Œå¹…"] if "æ¶¨è·Œå¹…" in row else 0), 2),
                        limit_status=0,  # é»˜è®¤éæ¶¨åœï¼Œåç»­é€šè¿‡ä¸“é—¨å‡½æ•°å›å¡«
                        limit_up_text=None,
                    )
                    session.add(daily_data)
                    total_saved += 1

        session.commit()

    logger.info(f"Saved {total_saved} daily market data records")
    return total_saved


def save_stock_basic_info(spot_data: pd.DataFrame):
    """ä¿å­˜è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯"""
    total_saved = 0
    
    with Session(engine) as session:
        for _, row in spot_data.iterrows():
            code = row["ä»£ç "]
            name = row["åç§°"] if "åç§°" in row else code
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = session.exec(
                select(StockBasicInfo).where(StockBasicInfo.code == code)
            ).first()
            
            if existing is None:
                stock_info = StockBasicInfo(
                    code=code,
                    name=name,
                    description=None,
                    tags=None
                )
                session.add(stock_info)
                total_saved += 1
            elif existing.name != name:
                # æ›´æ–°è‚¡ç¥¨åç§°
                existing.name = name
                existing.updated_at = datetime.now()
        
        session.commit()
    
    logger.info(f"Saved/updated {total_saved} stock basic info records")
    return total_saved


def backfill_limit_up_texts_using_ths(lookback_days: int = 180) -> int:
    """å›å¡«æœ€è¿‘è‹¥å¹²å¤©å†…æ‰€æœ‰äº¤æ˜“æ—¥çš„æ¶¨åœçŠ¶æ€å’Œæ–‡æœ¬ã€‚
    æ£€æŸ¥å“ªäº›äº¤æ˜“æ—¥æ²¡æœ‰å¤„ç†è¿‡æ¶¨åœä¿¡æ¯ï¼Œé€æ—¥è°ƒç”¨åŒèŠ±é¡ºæ¥å£è·å–æ¶¨åœæ•°æ®å¹¶æ›´æ–°æ•°æ®åº“ã€‚

    è¿”å›æ›´æ–°çš„è®°å½•æ€»æ•°ã€‚
    """
    from datetime import timedelta
    updated = 0
    # è·å–æœ€æ–°äº¤æ˜“æ—¥
    today, _ = get_latest_trade_date_and_limit_map()
    
    start_date = today - timedelta(days=lookback_days)

    with Session(engine) as session:
        # æ‰¾å‡ºæ•°æ®åº“ä¸­å­˜åœ¨çš„æ‰€æœ‰äº¤æ˜“æ—¥æœŸï¼ˆåœ¨æ—¶é—´çª—å£å†…ï¼‰
        all_trading_dates = session.exec(
            select(DailyMarketData.date).where(
                DailyMarketData.date >= start_date,
                DailyMarketData.date <= today
            ).group_by(DailyMarketData.date).order_by(DailyMarketData.date.asc())
        ).all()

        if not all_trading_dates:
            logger.info("No trading dates found in the specified window")
            return 0

        # æ£€æŸ¥å“ªäº›æ—¥æœŸæ²¡æœ‰limit_up_textæ•°æ®éœ€è¦å›å¡«
        # æ‰¾å‡ºå·²ç»æœ‰limit_up_textçš„æ‰€æœ‰æ—¥æœŸï¼ˆå»é‡ï¼‰
        dates_with_limit_text = session.exec(
            select(DailyMarketData.date).where(
                DailyMarketData.date >= start_date,
                DailyMarketData.date <= today,
                DailyMarketData.limit_up_text.is_not(None)
            ).group_by(DailyMarketData.date)
        ).all()
        
        dates_with_limit_text_set = set(dates_with_limit_text)
        
        # éœ€è¦å›å¡«çš„æ—¥æœŸ = æ‰€æœ‰äº¤æ˜“æ—¥æœŸ - å·²æœ‰limit_up_textçš„æ—¥æœŸ
        unprocessed_dates = [d for d in all_trading_dates if d not in dates_with_limit_text_set]

        if not unprocessed_dates:
            logger.info("All trading dates have been processed for limit-up status")
            return 0

        logger.info(f"Found {len(unprocessed_dates)} unprocessed trading date(s) for limit-up backfill")

        # é€æ—¥å¤„ç†æœªå¤„ç†çš„äº¤æ˜“æ—¥
        for d in unprocessed_dates:
            ds = d.strftime("%Y%m%d")  # THS APIéœ€è¦YYYYMMDDæ ¼å¼
            try:
                ths_df = uplimit10jqka(ds)
                limit_map = build_limit_up_map(ths_df)
            except Exception as e:
                logger.warning(f"Skip backfill for {ds} due to error: {e}")
                continue

            # è·å–è¯¥æ—¥æœŸçš„æ‰€æœ‰è‚¡ç¥¨è®°å½•
            day_records = session.exec(
                select(DailyMarketData).where(DailyMarketData.date == d)
            ).all()
            
            for record in day_records:
                if record.code in limit_map:
                    # æ¶¨åœè‚¡ç¥¨
                    record.limit_status = 1
                    record.limit_up_text = limit_map[record.code]
                    updated += 1
                else:
                    # éæ¶¨åœè‚¡ç¥¨
                    record.limit_status = 0
                    record.limit_up_text = None
                    
            session.commit()
            logger.info(f"Processed {len(day_records)} records for {ds}, found {len(limit_map)} limit-up stocks")

    logger.info(f"Backfilled limit-up data for {updated} record(s) across {len(unprocessed_dates)} day(s)")
    return updated


def load_daily_data_for_analysis(stock_codes: List[str], limit: int = 60) -> Dict[str, pd.DataFrame]:
    """ä»æ•°æ®åº“åŠ è½½æ—¥Kæ•°æ®ç”¨äºå› å­åˆ†æ"""
    history_data = {}
    
    print(f"ğŸ” load_daily_data_for_analysis: å¼€å§‹åŠ è½½ {len(stock_codes)} ä¸ªè‚¡ç¥¨çš„æ•°æ®")
    logger.info(f"load_daily_data_for_analysis: å¼€å§‹åŠ è½½ {len(stock_codes)} ä¸ªè‚¡ç¥¨çš„æ•°æ®")
    
    successful_count = 0
    failed_count = 0
    
    with Session(engine) as session:
        for i, code in enumerate(stock_codes):
            stmt = select(DailyMarketData).where(
                DailyMarketData.code == code
            ).order_by(DailyMarketData.date.desc()).limit(limit)
            
            daily_records = session.exec(stmt).all()
            if daily_records:
                df = pd.DataFrame([{
                    "æ—¥æœŸ": pd.to_datetime(record.date),
                    "å¼€ç›˜": record.open_price,
                    "æœ€é«˜": record.high_price,
                    "æœ€ä½": record.low_price,
                    "æ”¶ç›˜": record.close_price,
                    "æˆäº¤é‡": record.volume,
                    "æˆäº¤é¢": record.amount,
                    "æ¶¨è·Œå¹…": record.change_pct,
                    "limit_up_text": record.limit_up_text
                } for record in daily_records])
                df = df.sort_values("æ—¥æœŸ")
                history_data[code] = df
                successful_count += 1
            else:
                failed_count += 1
                if failed_count <= 5:  # åªæ‰“å°å‰5ä¸ªå¤±è´¥çš„
                    print(f"âŒ è‚¡ç¥¨ {code} æ²¡æœ‰æ‰¾åˆ°å†å²æ•°æ®")
            
            # æ¯å¤„ç†50ä¸ªè‚¡ç¥¨æ‰“å°ä¸€æ¬¡è¿›åº¦
            if (i + 1) % 50 == 0:
                print(f"ğŸ“Š å·²å¤„ç† {i + 1}/{len(stock_codes)} ä¸ªè‚¡ç¥¨ï¼ŒæˆåŠŸ {successful_count} ä¸ª")
    
    print(f"âœ… load_daily_data_for_analysis å®Œæˆï¼šæˆåŠŸåŠ è½½ {successful_count} ä¸ªè‚¡ç¥¨ï¼Œå¤±è´¥ {failed_count} ä¸ª")
    logger.info(f"Loaded daily data for {len(history_data)} stocks from database (æˆåŠŸ:{successful_count}, å¤±è´¥:{failed_count})")
    return history_data