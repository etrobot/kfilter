from __future__ import annotations
import logging
import threading
from datetime import datetime, date, timedelta
from typing import Optional, List, Dict, Any

import numpy as np
import pandas as pd
from sqlmodel import Session, select, func

from models import Task, TaskStatus, engine, DailyMarketData
from task_utils import (
    get_task, 
    update_task_progress,
)
from market_data import fetch_hot_spot, fetch_history, compute_factors, fetch_dragon_tiger_data
from .stock_data_manager import (
    save_daily_data,
    save_stock_basic_info,
    load_daily_data_for_analysis,
    save_spot_as_daily_data,
    backfill_limit_up_texts_using_ths
)
from .concept_service import get_stocks_sectors_from_extended_analysis
from market_data import (
    calculate_and_save_weekly_data,
    calculate_and_save_monthly_data
)

logger = logging.getLogger(__name__)


def get_latest_trade_date_and_setup(task_id: str) -> tuple[Any, bool]:
    """è·å–æœ€æ–°äº¤æ˜“æ—¥æœŸå¹¶è®¾ç½®ä»»åŠ¡çŠ¶æ€"""
    task = get_task(task_id)
    if not task:
        logger.error(f"Task {task_id} not found")
        return None, True
    
    task.status = TaskStatus.RUNNING
    update_task_progress(task_id, 0.0, "å¼€å§‹åˆ†æä»»åŠ¡")
    
    logger.info(f"Starting stock analysis...")
    
    # è·å–æœ€æ–°äº¤æ˜“æ—¥å’Œæ¶¨åœæ•°æ®ï¼ˆæ¯æ¬¡ä»»åŠ¡éƒ½é‡æ–°è·å–ï¼‰
    try:
        from .stock_data_manager import get_latest_trade_date_and_limit_map
        update_task_progress(task_id, 0.02, "è·å–æœ€æ–°äº¤æ˜“æ—¥å’Œæ¶¨åœæ•°æ®")
        
        # å¼ºåˆ¶é‡æ–°è·å–æœ€æ–°æ•°æ®ï¼Œä¸ä½¿ç”¨ç¼“å­˜
        latest_trade_date, _ = get_latest_trade_date_and_limit_map(use_cache=False)
        logger.info(f"Latest trade date: {latest_trade_date}")
        return latest_trade_date, False
    except Exception as e:
        error_msg = f"æ— æ³•è·å–æœ€æ–°äº¤æ˜“æ—¥æœŸï¼š{e}"
        logger.error(error_msg)
        task.status = TaskStatus.FAILED
        task.message = error_msg
        task.completed_at = datetime.now().isoformat()
        return None, True


def collect_spot_data_and_select_stocks(task_id: str, top_n: int, latest_trade_date) -> tuple[pd.DataFrame, List[str], bool]:
    """æ”¶é›†å®æ—¶æ•°æ®å¹¶ç­›é€‰çƒ­é—¨è‚¡ç¥¨"""
    update_task_progress(task_id, 0.05, f"è·å–å®æ—¶è¡Œæƒ…æ•°æ® {latest_trade_date}")
    spot = fetch_hot_spot()
    
    # ä¿å­˜è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
    update_task_progress(task_id, 0.1, "ä¿å­˜è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯")
    save_stock_basic_info(spot)
    
    stock_codes = spot["ä»£ç "].tolist()
    
    return spot, stock_codes, False


def check_and_upsert_spot_data(task_id: str,stock_codes: List[str], spot: pd.DataFrame, latest_trade_date) -> bool:
    """æ£€æŸ¥æ˜¯å¦éœ€è¦upsert spotæ•°æ®åˆ°æ—¥Kæ•°æ®åº“"""
    update_task_progress(task_id, 0.18, "æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°å½“æ—¥Kçº¿æ•°æ®")
    
    should_upsert_spot = False
    with Session(engine) as session:
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ€æ–°äº¤æ˜“æ—¥çš„æ•°æ®
        latest_data_count = session.exec(
            select(func.count(DailyMarketData.id))
            .where(DailyMarketData.date == latest_trade_date)
        ).first()
        # logger.info(f"Found {latest_data_count} records for latest_trade_date: {latest_trade_date}")
        
        # è·å–å‰ä¸€ä¸ªäº¤æ˜“æ—¥å¹¶æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        previous_trade_date = latest_trade_date - timedelta(days=3 if latest_trade_date.weekday() == 0 else 1)
        # logger.info(f"latest_trade_date: {latest_trade_date} (weekday: {latest_trade_date.weekday()}), calculated previous_trade_date: {previous_trade_date}")
        previous_data_count = session.exec(
            select(func.count(DailyMarketData.id))
            .where(DailyMarketData.date == previous_trade_date)
        ).first()

        # åªæœ‰å½“ä»Šå¤©æœ‰æ•°æ®ä¸”å‰ä¸€ä¸ªäº¤æ˜“æ—¥ä¹Ÿæœ‰æ•°æ®æ—¶ï¼Œæ‰è¿›è¡Œupsert
        if latest_data_count == 0:
            should_upsert_spot = False
            logger.info(f"No daily K data found for {latest_trade_date}, skipping spot data upsert, will fetch history instead")
        elif previous_data_count == 0:
            should_upsert_spot = False
            logger.info(f"No daily K data found for previous trading day {previous_trade_date}, skipping spot data upsert, will fetch history instead")
        else:
            should_upsert_spot = True
            logger.info(f"Found {latest_data_count} records for {latest_trade_date} and {previous_data_count} records for {previous_trade_date}, will upsert spot data")

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»£ç éƒ½æœ‰æ•°æ®
        has_all_codes_data = session.exec(
            select(func.count(DailyMarketData.id))
            .where(DailyMarketData.code.in_(stock_codes))
        ).all()
        logger.info(f"Found {has_all_codes_data} records for {stock_codes}")

        if len(has_all_codes_data) != len(stock_codes):
            should_upsert_spot = False
            logger.info(f"Not all codes have daily K data, will upsert spot data")
        
    if should_upsert_spot:
        # æ·»åŠ æ—¥æœŸåˆ—åˆ°spotæ•°æ®è¿›è¡Œupsert
        spot_with_date = spot.copy()
        spot_with_date["æ—¥æœŸ"] = latest_trade_date
        
        update_task_progress(task_id, 0.2, "ä¿å­˜å½“æ—¥å®æ—¶æ•°æ®ä¸ºKçº¿æ•°æ®")
        saved_count = save_spot_as_daily_data(spot_with_date)
        logger.info(f"Upserted {saved_count} spot records as daily K data for {latest_trade_date}")
    else:
        update_task_progress(task_id, 0.2, "è·³è¿‡spotæ•°æ®upsertï¼Œå°†é€šè¿‡fetch_historyè·å–æ•°æ®")
    
    return should_upsert_spot


def get_stocks_from_database(task_id: str, top_n: int) -> tuple[pd.DataFrame, List[str], bool]:
    """ä»æ•°æ®åº“è·å–è‚¡ç¥¨æ•°æ®ï¼ˆå½“ä¸æ”¶é›†æœ€æ–°æ•°æ®æ—¶ä½¿ç”¨ï¼‰"""
    
    update_task_progress(task_id, 0.15, "ä½¿ç”¨å†å²æ•°æ®è¿›è¡Œåˆ†æï¼ˆè·³è¿‡çƒ­ç‚¹æ•°æ®é‡‡é›†ï¼‰")
    
    # ä»æ•°æ®åº“è·å–è‚¡ç¥¨ä»£ç ï¼ˆæœ€è¿‘æœ‰è¶³å¤Ÿæ•°æ®çš„è‚¡ç¥¨ï¼‰
    with Session(engine) as session:
        # è·å–æœ‰è¶³å¤Ÿå†å²æ•°æ®çš„è‚¡ç¥¨ï¼ˆè‡³å°‘35å¤©ç”¨äºå› å­è®¡ç®—ï¼‰
        stocks_with_data = session.exec(
            select(DailyMarketData.code, func.count(DailyMarketData.id).label('record_count'))
            .group_by(DailyMarketData.code)
            .having(func.count(DailyMarketData.id) >= 35)  # å› å­è®¡ç®—çš„æœ€å°å€¼
            .order_by(func.count(DailyMarketData.id).desc())
            .limit(top_n * 2)  # è·å–æ›´å¤šå€™é€‰
        ).all()
        if stocks_with_data:
            # è·å–è¿™äº›è‚¡ç¥¨çš„æœ€æ–°æ—¥æœŸ
            candidate_codes = [code for code, _ in stocks_with_data]
            recent_date = session.exec(
                select(func.max(DailyMarketData.date))
                .where(DailyMarketData.code.in_(candidate_codes))
            ).first()
            
            if recent_date:
                # ä»å€™é€‰è‚¡ç¥¨ä¸­è·å–æœ€è¿‘äº¤æ˜“æ—¥æŒ‰æˆäº¤é¢æ’åºçš„è‚¡ç¥¨ï¼ˆä¸ä¾èµ–StockBasicInfoï¼‰
                recent_stocks = session.exec(
                    select(DailyMarketData.code, DailyMarketData.amount)
                    .where(
                        DailyMarketData.date == recent_date,
                        DailyMarketData.code.in_(candidate_codes)
                    )
                    .order_by(DailyMarketData.amount.desc())
                ).all()
                
                if recent_stocks:
                    top_spot = pd.DataFrame([
                        {"ä»£ç ": code, "åç§°": code, "æˆäº¤é¢": amount}
                        for code, amount in recent_stocks
                    ])
                    stock_codes = top_spot["ä»£ç "].tolist()
                    logger.info(f"Selected top {len(top_spot)} stocks with sufficient data from database (date: {recent_date})")
                    return top_spot, stock_codes, False
                else:
                    # åå¤‡æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨å€™é€‰è‚¡ç¥¨ä»£ç 
                    top_spot = pd.DataFrame([
                        {"ä»£ç ": code, "åç§°": code}
                        for code in candidate_codes
                    ])
                    stock_codes = top_spot["ä»£ç "].tolist()
                    logger.info(f"Using fallback: selected {len(top_spot)} stocks with sufficient data")
                    return top_spot, stock_codes, False
            else:
                raise Exception("No recent data found for stocks with sufficient history.")
        else:
            raise Exception("No stocks found with sufficient historical data (>=35 days). Please run with 'collect_latest_data=True' first.")


def fetch_and_save_historical_data(task_id: str, stock_codes: List[str], should_upsert_spot: bool, collect_latest_data: bool, latest_trade_date: date, stop_event: Optional[threading.Event] = None) -> bool:
    """è·å–å¹¶ä¿å­˜å†å²æ•°æ® - æ”¹è¿›ä¸ºæ‰¹é‡å¤„ç†ï¼Œè·å–ä¸€æ‰¹å­˜ä¸€æ‰¹"""
    if collect_latest_data:
        if not should_upsert_spot:
            # å¦‚æœæ²¡æœ‰æœ€æ–°äº¤æ˜“æ—¥æ•°æ®ï¼Œè·å–å†å²æ•°æ®è¿›è¡Œå›å¡«
            update_task_progress(task_id, 0.25, "ä»å¤–éƒ¨APIåˆ†æ‰¹è·å–å†å²æ•°æ®")
            
            end_date_str = latest_trade_date.strftime("%Y%m%d")
            total_stocks = len(stock_codes)
            
            logger.info(f"å¼€å§‹é€ä¸ªè·å–å†å²æ•°æ®ï¼Œæ€»å…± {total_stocks} ä¸ªè‚¡ç¥¨")
            
            successful_count = 0
            failed_count = 0
            
            for i, stock_code in enumerate(stock_codes):
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
                if stop_event and stop_event.is_set():
                    logger.info(f"ä»»åŠ¡è¢«å–æ¶ˆï¼Œå·²å¤„ç† {successful_count} ä¸ªè‚¡ç¥¨")
                    return True
                
                # æ›´æ–°è¿›åº¦
                progress = 0.25 + (0.1 * i / total_stocks)  # ä»0.25åˆ°0.35
                update_task_progress(task_id, progress, f"è·å–ç¬¬ {i+1}/{total_stocks} ä¸ªè‚¡ç¥¨å†å²æ•°æ®: {stock_code}")
                
                try:
                    # è·å–å•ä¸ªè‚¡ç¥¨çš„å†å²æ•°æ®ï¼ˆä¸ä¼ é€’task_idé¿å…å†…éƒ¨è¿›åº¦æ˜¾ç¤ºå¹²æ‰°ï¼‰
                    stock_history = fetch_history([stock_code], end_date=end_date_str, days=365, task_id=None)
                    
                    if stock_history:
                        # ç«‹å³ä¿å­˜å•ä¸ªè‚¡ç¥¨çš„æ•°æ®
                        save_daily_data(stock_history)
                        logger.info(f"ç¬¬ {i+1}/{total_stocks} ä¸ªè‚¡ç¥¨ {stock_code} å†å²æ•°æ®ä¿å­˜å®Œæˆï¼ŒåŒ…å« {len(stock_history)} æ¡è®°å½•")
                        successful_count += 1
                    else:
                        logger.warning(f"ç¬¬ {i+1}/{total_stocks} ä¸ªè‚¡ç¥¨ {stock_code} æœªè·å–åˆ°å†å²æ•°æ®")
                        failed_count += 1
                        
                except Exception as e:
                    logger.error(f"ç¬¬ {i+1}/{total_stocks} ä¸ªè‚¡ç¥¨ {stock_code} å†å²æ•°æ®è·å–/ä¿å­˜å¤±è´¥: {e}")
                    failed_count += 1
                    # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè‚¡ç¥¨ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
                    continue
            
            update_task_progress(task_id, 0.35, f"å†å²æ•°æ®è·å–å®Œæˆï¼ŒæˆåŠŸ {successful_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª")
            logger.info(f"å†å²æ•°æ®è·å–å®Œæˆï¼šæˆåŠŸ {successful_count}/{total_stocks} ä¸ªè‚¡ç¥¨")
            
            # å¦‚æœå¤±è´¥è‚¡ç¥¨è¿‡å¤šï¼Œè®°å½•è­¦å‘Š
            if failed_count > successful_count:
                logger.warning(f"å†å²æ•°æ®è·å–å¤±è´¥è‚¡ç¥¨è¾ƒå¤š: {failed_count}/{total_stocks}")
            
            # åªè¦æœ‰æˆåŠŸçš„è‚¡ç¥¨å°±ç»§ç»­ï¼Œä¸å› ä¸ºéƒ¨åˆ†å¤±è´¥è€Œç»ˆæ­¢
            if successful_count == 0:
                logger.error("æ‰€æœ‰è‚¡ç¥¨éƒ½è·å–å¤±è´¥ï¼Œæ— æ³•ç»§ç»­åˆ†æ")
                return True  # è¿”å›é”™è¯¯
        else:
            # å¦‚æœupsertäº†spotæ•°æ®ï¼Œè·³è¿‡å¤–éƒ¨APIè°ƒç”¨ï¼Œå› ä¸ºæˆ‘ä»¬æœ‰å½“å‰æ•°æ®
            update_task_progress(task_id, 0.35, "è·³è¿‡å¤–éƒ¨APIè°ƒç”¨ï¼ˆå·²upsertå½“æ—¥spotæ•°æ®ï¼‰")
            logger.info("Spot data upserted, skipping external API fetch for historical data")
    else:
        # ä¸æ”¶é›†æœ€æ–°æ•°æ®æ—¶è·³è¿‡å¤–éƒ¨APIè°ƒç”¨
        update_task_progress(task_id, 0.35, "ä½¿ç”¨æ•°æ®åº“ä¸­çš„å†å²æ•°æ®ï¼ˆè·³è¿‡å¤–éƒ¨APIè°ƒç”¨ï¼‰")
        logger.info("Using existing database historical data, skipping external API fetch")
    
    return False


def backfill_limit_up_data(task_id: str) -> bool:
    """å›å¡«å†å²æ¶¨åœæ¿ç±»å‹"""
    try:
        update_task_progress(task_id, 0.45, "å›å¡«å†å²æ¶¨åœæ¿ç±»å‹")
        backfilled = backfill_limit_up_texts_using_ths(lookback_days=180)
        logger.info(f"Backfilled {backfilled} limit_up_text records in recent history")
        return False
    except Exception as e:
        logger.warning(f"Skip backfilling limit-up texts due to error: {e}")
        return False


def calculate_weekly_monthly_data(task_id: str, stock_codes: List[str], should_upsert_spot: bool, collect_latest_data: bool) -> bool:
    """è®¡ç®—å¹¶ä¿å­˜å‘¨Kçº¿å’ŒæœˆKçº¿æ•°æ®"""
    if collect_latest_data:
        if not should_upsert_spot:
            # å½“æˆ‘ä»¬è·å–äº†å†å²æ•°æ®æ—¶è®¡ç®—å‘¨Kçº¿/æœˆKçº¿æ•°æ®
            # Step 5a: è®¡ç®—å¹¶ä¿å­˜å‘¨Kçº¿æ•°æ®
            update_task_progress(task_id, 0.5, "è®¡ç®—å¹¶ä¿å­˜å‘¨Kçº¿æ•°æ®")
            calculate_and_save_weekly_data(stock_codes, task_id)
            
            # Step 5b: è®¡ç®—å¹¶ä¿å­˜æœˆKçº¿æ•°æ®
            update_task_progress(task_id, 0.6, "è®¡ç®—å¹¶ä¿å­˜æœˆKçº¿æ•°æ®")
            calculate_and_save_monthly_data(stock_codes, task_id)
        else:
            # å½“æˆ‘ä»¬åªupsertäº†spotæ•°æ®æ—¶è·³è¿‡å‘¨Kçº¿/æœˆKçº¿è®¡ç®—
            update_task_progress(task_id, 0.6, "è·³è¿‡å‘¨Kçº¿å’ŒæœˆKçº¿è®¡ç®—ï¼ˆä»…upsertäº†spotæ•°æ®ï¼‰")
            logger.info("Skipping weekly/monthly data calculation since only spot data was upserted")
    else:
        # ä½¿ç”¨ç°æœ‰æ•°æ®æ—¶è·³è¿‡å‘¨Kçº¿/æœˆKçº¿è®¡ç®—
        update_task_progress(task_id, 0.6, "ä½¿ç”¨ç°æœ‰æ•°æ®ï¼Œè·³è¿‡å‘¨Kçº¿å’ŒæœˆKçº¿è®¡ç®—")
        logger.info("Skipping weekly/monthly data calculation since using existing database data")
    
    return False


def compute_factors_and_analysis(task_id: str, stock_codes: List[str], 
                                latest_trade_date, selected_factors: Optional[List[str]] = None) -> Dict[str, Any]:
    """è®¡ç®—å› å­å¹¶è¿›è¡Œåˆ†æ"""
    # Step 7: ä»æ•°æ®åº“åŠ è½½æ•°æ®è¿›è¡Œå› å­è®¡ç®—
    update_task_progress(task_id, 0.7, "ä»æ•°æ®åº“åŠ è½½æ•°æ®è¿›è¡Œå› å­è®¡ç®—")
    
    print(f"ğŸ” compute_factors_and_analysis: åˆ†æ {len(stock_codes)} ä¸ªè‚¡ç¥¨")
    
    # ä»æ•°æ®åº“åŠ è½½å†å²æ•°æ®ç”¨äºå› å­è®¡ç®—
    history_for_factors = load_daily_data_for_analysis(stock_codes, limit=120)
    
    # ç›´æ¥ä»æ•°æ®åº“æ„å»ºæ‰€æœ‰è‚¡ç¥¨çš„spotæ•°æ®
    print(f"ğŸ”§ ä»æ•°æ®åº“æ„å»º {len(stock_codes)} ä¸ªè‚¡ç¥¨çš„spotæ•°æ®...")
    
    from models import StockBasicInfo
    from sqlmodel import Session, select
    
    complete_spot_data = []
    
    # ä»æ•°æ®åº“è·å–æ‰€æœ‰è‚¡ç¥¨çš„åŸºæœ¬ä¿¡æ¯å’Œæœ€æ–°ä»·æ ¼
    with Session(engine) as session:
        for code in stock_codes:
            # è·å–è‚¡ç¥¨åç§°
            stock_info = session.exec(
                select(StockBasicInfo.name).where(StockBasicInfo.code == code)
            ).first()
            
            # è·å–æœ€æ–°ä»·æ ¼å’Œæˆäº¤é¢
            latest_data = session.exec(
                select(DailyMarketData.close_price, DailyMarketData.amount)
                .where(DailyMarketData.code == code)
                .order_by(DailyMarketData.date.desc())
                .limit(1)
            ).first()
            
            complete_spot_data.append({
                "ä»£ç ": code,
                "åç§°": stock_info or code,
                "æœ€æ–°ä»·": latest_data[0] if latest_data else 0,
                "æˆäº¤é¢": latest_data[1] if latest_data else 0
            })
    
    # åˆ›å»ºå®Œæ•´çš„DataFrame
    complete_spot = pd.DataFrame(complete_spot_data)
    print(f"âœ… æ„å»ºçš„spotæ•°æ®åŒ…å« {len(complete_spot)} ä¸ªè‚¡ç¥¨")
    
    # Step 8: è®¡ç®—å› å­
    factor_msg = f"è®¡ç®—{'é€‰å®š' if selected_factors else 'æ‰€æœ‰'}å› å­"
    update_task_progress(task_id, 0.85, factor_msg)
    df = compute_factors(complete_spot, history_for_factors, task_id=task_id, selected_factors=selected_factors)
    
    update_task_progress(task_id, 0.95, "æ•°æ®æ¸…ç†å’Œæ ¼å¼åŒ–")
    
    # æ¸…ç†æ•°æ®ç”¨äºJSONåºåˆ—åŒ–
    if not df.empty:
        # å°†NaNå€¼æ›¿æ¢ä¸ºNone
        df = df.replace({np.nan: None})
        # ç¡®ä¿æ‰€æœ‰æ•°å€¼éƒ½æ­£ç¡®æ ¼å¼åŒ–
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            df[col] = df[col].astype(float, errors='ignore')
    
    data = df.to_dict(orient="records") if not df.empty else []
    
    # æ·»åŠ æ¿å—ä¿¡æ¯å’Œæ’åå‰ç¼€ï¼ˆä»æ‰©å±•åˆ†æç»“æœè·å–ï¼‰
    if data:
        update_task_progress(task_id, 0.97, "æ·»åŠ æ¿å—ä¿¡æ¯å’Œæ’åå‰ç¼€")
        stock_codes_for_sectors = [record.get('ä»£ç ') for record in data if 'ä»£ç ' in record]
        sectors_map = get_stocks_sectors_from_extended_analysis(stock_codes_for_sectors)
        
        # ä¸ºæ¯æ¡è®°å½•æ·»åŠ æ‰€å±æ¿å—ï¼ˆå¸¦æ’åå‰ç¼€ï¼‰
        for record in data:
            stock_code = record.get('ä»£ç ')
            if stock_code and stock_code in sectors_map:
                sector_name, rank = sectors_map[stock_code]
                # åœ¨æ‰€å±æ¿å—å­—æ®µä¸­æ·»åŠ æ’åå‰ç¼€ï¼ˆå¦‚ï¼š#01è‹±ä¼Ÿè¾¾æ¦‚å¿µï¼‰
                record['æ‰€å±æ¿å—'] = f"{rank:02d}-{sector_name}"

    return {
        "data": data,
        "count": len(data),
    }


def complete_analysis_task(task_id: str, result: Dict[str, Any]) -> None:
    """å®Œæˆåˆ†æä»»åŠ¡"""
    import json
    import os
    from task_utils import set_last_completed_task
    from .services import ANALYSIS_RESULTS_CACHE, CACHE_LOCK
    
    task = get_task(task_id)
    if not task:
        return
    
    # å®Œæˆä»»åŠ¡
    task.status = TaskStatus.COMPLETED
    task.progress = 1.0
    task.message = f"åˆ†æå®Œæˆï¼Œæ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“ï¼Œå…± {result['count']} æ¡ç»“æœ"
    task.completed_at = datetime.now().isoformat()
    task.result = result

    # Prepare full result data for JSON and cache
    full_result = {
        "task_id": task_id,
        "status": task.status.value,
        "progress": task.progress,
        "message": task.message,
        "completed_at": task.completed_at,
        "created_at": task.created_at,
        "top_n": task.top_n,
        "selected_factors": task.selected_factors,
        "data": result["data"],
        "count": result["count"],
        "from_cache": False
    }

    # Store results in memory cache for frontend access
    with CACHE_LOCK:
        ANALYSIS_RESULTS_CACHE[task_id] = full_result.copy()

    # Save results to JSON file for persistence across server restarts
    try:
        json_file_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "ranking.json")
        with open(json_file_path, 'w', encoding='utf-8') as f:
            json.dump(full_result, f, ensure_ascii=False, indent=2)
        logger.info(f"Analysis results saved to {json_file_path}")
    except Exception as e:
        logger.warning(f"Failed to save analysis results to JSON file: {e}")

    set_last_completed_task(task)
    logger.info(f"Analysis completed successfully with database integration. Found {result['count']} results")


def run_analysis_task(task_id: str, top_n: int, selected_factors: Optional[List[str]] = None, 
                     collect_latest_data: bool = True, stop_event: Optional[threading.Event] = None):
    """ä¸»è¦çš„åˆ†æä»»åŠ¡è¿è¡Œå™¨"""
    
    def check_cancel() -> bool:
        if stop_event is not None and stop_event.is_set():
            task = get_task(task_id)
            if task:
                task.status = TaskStatus.CANCELLED
                task.message = "ä»»åŠ¡å·²å–æ¶ˆ"
                task.completed_at = datetime.now().isoformat()
            logger.info(f"Task {task_id} cancelled by user")
            return True
        return False
    
    # Step 1: è·å–æœ€æ–°äº¤æ˜“æ—¥æœŸå¹¶è®¾ç½®ä»»åŠ¡
    latest_trade_date, has_error = get_latest_trade_date_and_setup(task_id)
    if has_error or check_cancel():
        return

    # åˆå§‹åŒ–æ˜¯å¦éœ€è¦upsert spotæ•°æ®çš„æ ‡å¿—
    should_upsert_spot = False

    if collect_latest_data:
        # Step 2: æ”¶é›†å®æ—¶æ•°æ®å¹¶ç­›é€‰è‚¡ç¥¨
        top_spot, stock_codes, has_error = collect_spot_data_and_select_stocks(task_id, top_n, latest_trade_date)
        logger.info(f"çƒ­ç‚¹è‚¡ç¥¨æ•°é‡: {len(stock_codes)}")
        
        dragon_tiger_data = fetch_dragon_tiger_data(
            page_number=1, page_size=100, statistics_cycle="04"
        )
        dragon_tiger_codes = dragon_tiger_data["ä»£ç "].tolist()
        logger.info(f"é¾™è™æ¦œè‚¡ç¥¨æ•°é‡: {len(dragon_tiger_codes)}")
        
        # ä¿å­˜é¾™è™æ¦œè‚¡ç¥¨çš„åŸºæœ¬ä¿¡æ¯åˆ°StockBasicInfo
        save_stock_basic_info(dragon_tiger_data)
        
        # åˆå¹¶å‰è®°å½•æ€»æ•°
        total_before_dedup = len(stock_codes) + len(dragon_tiger_codes)
        logger.info(f"åˆå¹¶å‰æ€»è‚¡ç¥¨æ•°: {total_before_dedup} (çƒ­ç‚¹:{len(stock_codes)} + é¾™è™æ¦œ:{len(dragon_tiger_codes)})")
        
        # åˆå¹¶å¹¶å»é‡
        stock_codes = list(set(stock_codes + dragon_tiger_codes))
        logger.info(f"å»é‡åæœ€ç»ˆè‚¡ç¥¨æ•°: {len(stock_codes)} (å»é™¤äº† {total_before_dedup - len(stock_codes)} ä¸ªé‡å¤)")
        print('number:', len(stock_codes))
        
        if has_error or check_cancel():
            return
        
        # Step 3: æ£€æŸ¥å¹¶upsert spotæ•°æ®
        should_upsert_spot = check_and_upsert_spot_data(task_id, stock_codes, top_spot, latest_trade_date)
        if check_cancel():
            return
    else:
        # è·³è¿‡çƒ­ç‚¹æ•°æ®æ”¶é›†ï¼Œä½¿ç”¨æ•°æ®åº“ä¸­çš„ç°æœ‰æ•°æ®
        top_spot, stock_codes, has_error = get_stocks_from_database(task_id, top_n)
        if has_error or check_cancel():
            return

    
    # Step 4: è·å–å†å²æ•°æ®
    has_error = fetch_and_save_historical_data(task_id, stock_codes, should_upsert_spot, collect_latest_data, latest_trade_date, stop_event)
    if has_error or check_cancel():
        return

    update_task_progress(task_id, 0.4, "å†å²æ•°æ®æ›´æ–°å®Œæˆ")
    if check_cancel():
        return

    # Step 5: å›å¡«æ¶¨åœæ¿ç±»å‹
    if collect_latest_data:
        has_error = backfill_limit_up_data(task_id)
        if has_error or check_cancel():
            return

    # Step 6: è®¡ç®—å‘¨Kçº¿å’ŒæœˆKçº¿æ•°æ®
    has_error = calculate_weekly_monthly_data(task_id, stock_codes, should_upsert_spot, collect_latest_data)
    if has_error or check_cancel():
        return
    
    # Step 7-8: è®¡ç®—å› å­å¹¶è¿›è¡Œåˆ†æ
    result = compute_factors_and_analysis(task_id, stock_codes, latest_trade_date, selected_factors)
    if check_cancel():
        return

    # Step 9: å®Œæˆä»»åŠ¡
    complete_analysis_task(task_id, result)